"use client";

import React, { useRef } from "react";
import type { Turn } from "../page";

export function InterviewPanel(props: {
  status: "idle" | "connecting" | "live" | "done" | "error";
  setStatus: (s: any) => void;
  onFinalTurn: (t: Turn) => void;
  onOutline: (o: any) => void;
  setRecordingUrl: (url: string) => void;
  setRecordingBlob: (b: Blob | null) => void;
  mode:"realtime"|"fallback";
  setMode:(m:"realtime"|"fallback")=>void;
  turns: Turn[];
}) {
  const streamRef = useRef<MediaStream | null>(null);
  const pcRef = useRef<RTCPeerConnection | null>(null);
  const remoteAudioRef = useRef<HTMLAudioElement | null>(null);
  const dataChannelRef = useRef<RTCDataChannel | null>(null);
  const aiTextBufferRef = useRef<string>("");
  const recorderRef = useRef<MediaRecorder | null>(null);
const recordedChunksRef = useRef<BlobPart[]>([]);
const mixCtxRef = useRef<AudioContext | null>(null);
const mixDestRef = useRef<MediaStreamAudioDestinationNode | null>(null);
const remoteMixedRef = useRef(false);
const recognitionRef = useRef<any>(null);



  const statusMap: Record<string, string> = {
    idle: "æœªå¼€å§‹",
    connecting: "è¿æ¥ä¸­",
    live: "å½•åˆ¶ä¸­",
    done: "å·²ç»“æŸ",
    error: "å‡ºé”™",
  };

  const canStart = props.status === "idle" || props.status === "done" || props.status === "error";
  const canStop = props.status === "live" || props.status === "connecting";
  

  async function startRealtime() {

    // 1) è·å–éº¦å…‹é£
    const stream = await navigator.mediaDevices.getUserMedia({
      audio: {
        echoCancellation: true,
        noiseSuppression: true,
        autoGainControl: true,
      },
    });
    
    streamRef.current = stream;
    //  å»ºç«‹æ··éŸ³ä¸Šä¸‹æ–‡ï¼ˆåªå»ºä¸€æ¬¡ï¼‰
if (!mixCtxRef.current) {
  mixCtxRef.current = new AudioContext();
  mixDestRef.current = mixCtxRef.current.createMediaStreamDestination();
}
const ctx = mixCtxRef.current!;
const dest = mixDestRef.current!;

// æŠŠéº¦å…‹é£æ¥å…¥æ··éŸ³
const micSource = ctx.createMediaStreamSource(stream);
micSource.connect(dest);

//  å¯åŠ¨å½•éŸ³ï¼šå½•çš„æ˜¯â€œæ··éŸ³åçš„æµâ€
recordedChunksRef.current = [];
props.setRecordingBlob(null);
props.setRecordingUrl("");

const preferTypes = [
  "audio/webm;codecs=opus",
  "audio/webm",
  "audio/mp4",
];
const mimeType = preferTypes.find((t) => MediaRecorder.isTypeSupported(t));
const rec = mimeType ? new MediaRecorder(dest.stream, { mimeType }) : new MediaRecorder(dest.stream);
console.log("ğŸ™ MediaRecorder mimeType =", rec.mimeType);


recorderRef.current = rec;

rec.ondataavailable = (e) => {
  if (e.data && e.data.size > 0) recordedChunksRef.current.push(e.data);
};

rec.onstop = () => {
  const blob = new Blob(recordedChunksRef.current, { type: "audio/webm" });
  props.setRecordingBlob(blob);
  const url = URL.createObjectURL(blob);
  props.setRecordingUrl(url);
  console.log(" å½•éŸ³å®Œæˆ:", blob.size, "bytes");
};


// console.log("âº å¼€å§‹å½•éŸ³ï¼ˆæ··éŸ³æµï¼‰");
console.log("âº å½•éŸ³å™¨å·²å°±ç»ªï¼Œç­‰å¾… AI éŸ³è½¨åè‡ªåŠ¨å¼€å§‹");

rec.start();
    // 2) å»º WebRTC peer connection
    const pc = new RTCPeerConnection();

    pcRef.current = pc;
// â­ åˆ›å»º data channelï¼ˆç”¨æ¥æ¥æ”¶é€å­—å­—å¹•ç­‰äº‹ä»¶ï¼‰

    aiTextBufferRef.current = "";

    const dc = pc.createDataChannel("oai-events");

    dataChannelRef.current = dc;

    dc.onopen = () => {
      console.log("ğŸ“¡ Data channel open");
    };

    dc.onmessage = (ev) => {
      try {
        const msg = JSON.parse(ev.data);

        // 1) ç”¨æˆ·è¯­éŸ³è½¬å†™å®Œæˆï¼šæ˜¾ç¤ºã€Œä½ ï¼š...ã€
        // æ–‡æ¡£ï¼šconversation.item.input_audio_transcription.completed 
        if (msg.type === "conversation.item.input_audio_transcription.completed" && msg.transcript) {
          props.onFinalTurn({ role: "user", text: String(msg.transcript) });
          return;
        }

    // 2) AI å­—å¹•å¢é‡ï¼štext æˆ– audio_transcript éƒ½æ‹¼åˆ°åŒä¸€ä¸ª buffer
      if (
        (msg.type === "response.output_text.delta" && msg.delta) ||
        (msg.type === "response.output_audio_transcript.delta" && msg.delta)
      ) {
        aiTextBufferRef.current += String(msg.delta);
        return;
      }

// 3) AI å­—å¹•å®Œæˆï¼štext æˆ– audio_transcript å®Œæˆå°±è½åœ°
      if (
        msg.type === "response.output_text.done" ||
        msg.type === "response.output_text.completed" ||
        msg.type === "response.output_audio_transcript.done" ||
        msg.type === "response.output_audio_transcript.completed"
      ) {
        const finalText = aiTextBufferRef.current.trim();
        if (finalText) props.onFinalTurn({ role: "assistant", text: finalText });
        aiTextBufferRef.current = "";
        return;
      }


    // å…œåº•ï¼šä½ æƒ³çœ‹æ‰€æœ‰äº‹ä»¶ç»§ç»­è°ƒè¯•å°±æ‰“å¼€è¿™ä¸€è¡Œ
    // console.log("ğŸ“¨ Realtime event:", msg);
  } catch (e) {
    // å¦‚æœå¶å°”ä¸æ˜¯ JSONï¼Œå°±å¿½ç•¥
    // console.log("Non-JSON event:", ev.data);
  }
};

    // 3) æŠŠéº¦å…‹é£éŸ³è½¨å‘ç»™ Realtime
    for (const track of stream.getTracks()) {
      pc.addTrack(track, stream);
    }

    // 4) æ¥æ”¶ AI çš„è¿œç«¯éŸ³è½¨å¹¶æ’­æ”¾
    pc.ontrack = (ev) => {
      const [remoteStream] = ev.streams;
    
      // âœ… æ’­æ”¾ AI è¯­éŸ³
      if (remoteAudioRef.current) {
        remoteAudioRef.current.srcObject = remoteStream;
        remoteAudioRef.current.play().catch(() => {});
      }
    
      // âœ… æŠŠ AI è¿œç«¯éŸ³é¢‘æ¥å…¥æ··éŸ³ï¼ˆåªè¦ mixCtx å·²å­˜åœ¨ï¼‰
      if (!remoteMixedRef.current && mixCtxRef.current && mixDestRef.current) {
        remoteMixedRef.current = true;
        const ctx = mixCtxRef.current;
        const dest = mixDestRef.current;
        const remoteSource = ctx.createMediaStreamSource(remoteStream);
        remoteSource.connect(dest);
        console.log("ğŸ› å·²å°† AI éŸ³é¢‘æ¥å…¥æ··éŸ³");
      }

      // âœ… ç¬¬ä¸€æ¬¡æ‹¿åˆ° AI éŸ³è½¨æ—¶å†å¼€å§‹å½•éŸ³ï¼Œé¿å…æ¼å½• AI
if (recorderRef.current && recorderRef.current.state === "inactive") {
  recorderRef.current.start();
  console.log("âº å¼€å§‹å½•éŸ³ï¼ˆå·²æ¥å…¥ AI + éº¦å…‹é£ï¼‰");
}


    };
    

    // 5) ç”Ÿæˆ offer â†’ å‘ç»™ä½ è‡ªå·±çš„åç«¯ â†’ æ‹¿ answer
    const offer = await pc.createOffer();
    await pc.setLocalDescription(offer);

    const resp = await fetch("/api/realtime-session", {
      method: "POST",
      headers: { "Content-Type": "application/sdp" },
      body: offer.sdp,
    });

    if (!resp.ok) {
      const errText = await resp.text();
      throw new Error(errText);
    }

    const answerSdp = await resp.text();
    await pc.setRemoteDescription({ type: "answer", sdp: answerSdp });

    pc.onconnectionstatechange = () => {
      console.log("webrtc connectionState:", pc.connectionState);
    };
  }
  function speakZh(text: string) {
    try {
      window.speechSynthesis.cancel(); // åœæ‰ä¸Šä¸€æ¬¡
      const u = new SpeechSynthesisUtterance(text);
      u.lang = "zh-CN";
      u.rate = 1.05; // ç¨å¾®å¿«ä¸€ç‚¹æ›´åƒæ’­å®¢å¯¹è°ˆ
      u.pitch = 1.0;
  
      // å°½é‡é€‰ä¸­æ–‡å£°éŸ³ï¼ˆMac/Chrome ä¼šæœ‰å¤šä¸ª voiceï¼‰
      const voices = window.speechSynthesis.getVoices();
      const zh = voices.find((v) => v.lang?.toLowerCase().startsWith("zh"));
      if (zh) u.voice = zh;
  
      window.speechSynthesis.speak(u);
    } catch (e) {
      console.warn("speechSynthesis failed", e);
    }
  }
  
  function startFallbackSTT() {
    const SpeechRecognition =
      (window as any).SpeechRecognition ||
      (window as any).webkitSpeechRecognition;
  
    if (!SpeechRecognition) {
      alert("å½“å‰æµè§ˆå™¨ä¸æ”¯æŒè¯­éŸ³è¯†åˆ«ï¼ˆè¯·ä½¿ç”¨ Chromeï¼‰");
      return;
    }
  
    const rec = new SpeechRecognition();
    rec.lang = "zh-CN";
    rec.continuous = true;
    rec.interimResults = false;
  
    rec.onresult = async (e: any) => {
      const last = e.results[e.results.length - 1];
      const text = last[0].transcript.trim();
      if (!text) return;
  
      // â­ æŠŠã€Œä½ è¯´çš„è¯ã€å†™è¿›é€å­—ç¨¿
      props.onFinalTurn({ role: "user", text });

try {
  // è°ƒåç«¯æ‹¿ AI å›å¤
  const resp = await fetch("/api/fallback-reply", {
    method: "POST",
    headers: { "Content-Type": "application/json" },
    body: JSON.stringify({
      turns: props.turns,   // ç°æœ‰å¯¹è¯
      userText: text,       // æœ¬è½®ç”¨æˆ·è¯
    }),
  });

  const data = await resp.json();
  const aiText = String(data?.text || "").trim();

  if (aiText) {
    props.onFinalTurn({ role: "assistant", text: aiText });
    speakZh(aiText); // âœ… TTS æ’­æ”¾
  }
} catch (e) {
  console.error("fallback reply failed", e);
}

    };
  
    rec.onerror = (e: any) => {
      console.error("SpeechRecognition error", e);
    };
  
    rec.start();
    recognitionRef.current = rec;
    console.log("ğŸ¤ fallback è¯­éŸ³è¯†åˆ«å·²å¯åŠ¨");
  }
  
  function stopAll() {
    try {
      if (recorderRef.current && recorderRef.current.state !== "inactive") {
        recorderRef.current.stop();
      }
      recorderRef.current = null;
    } catch {}
    
    try {
      dataChannelRef.current?.close();
      dataChannelRef.current = null;
    } catch {}
  
    aiTextBufferRef.current = "";
  
    try {
      pcRef.current?.close();
      pcRef.current = null;
    } catch {}
    try {
      mixCtxRef.current?.close();
      mixCtxRef.current = null;
      mixDestRef.current = null;
    } catch {}
    try {
      recognitionRef.current?.stop();
      recognitionRef.current = null;
    } catch {}
    try {
      window.speechSynthesis.cancel();
    } catch {}
    
    try {
      streamRef.current?.getTracks().forEach((t) => t.stop());
      streamRef.current = null;
    } catch {}
    remoteMixedRef.current = false;

  }
  

  return (
    <div>
    {/* â­ fallback æ¨¡å¼æç¤ºï¼ˆåªåœ¨ fallback æ—¶æ˜¾ç¤ºï¼‰ */}
    {props.mode === "fallback" && (
      <div
        style={{
          padding: "8px 12px",
          borderRadius: 10,
          background: "rgba(255, 204, 0, 0.15)",
          fontSize: 12,
          marginBottom: 8,
        }}
      >
        ç½‘ç»œæ³¢åŠ¨ï¼Œå·²åˆ‡æ¢åˆ°ç¨³å®šæ¨¡å¼ï¼ˆä¸å½±å“å¯¹è°ˆä¸å¯¼å‡ºï¼‰
      </div>
    )}

      {/* éšè— audioï¼Œç”¨æ¥æ’­æ”¾ AI è¯­éŸ³ */}
      <audio ref={remoteAudioRef} autoPlay />

      {/* é¡¶éƒ¨ï¼šæ ‡é¢˜ + æŒ‰é’® + çŠ¶æ€ */}
      <div style={{ display: "flex", justifyContent: "space-between", alignItems: "center", gap: 12 }}>
        <div>
          <div style={{ fontWeight: 700, fontSize: 16 }}>å®æ—¶å¯¹è°ˆ</div>
          <div style={{ fontSize: 12, opacity: 0.6 }}>éº¦å…‹é£å¼€å§‹ / ç»“æŸ Â· è¯­éŸ³å¯¹è°ˆ</div>
        </div>

        <div style={{ display: "flex", alignItems: "center", gap: 10 }}>
          <button
            disabled={!canStart}
            onClick={async () => {
              props.setStatus("connecting");
              try {
                await startRealtime();
                props.setStatus("live");
              } catch (e) {
                console.error(e);
                stopAll();
                props.setMode("fallback");
                props.setStatus("live");
                startFallbackSTT(); 
              }
            }}
            style={{
              padding: "10px 14px",
              borderRadius: 12,
              border: "1px solid rgba(0,0,0,0.12)",
              background: canStart ? "white" : "rgba(0,0,0,0.05)",
              cursor: canStart ? "pointer" : "not-allowed",
              fontSize: 13,
              fontWeight: 600,
            }}
          >
            ğŸ™ï¸ å¼€å§‹
          </button>

          <button
            disabled={!canStop}
            onClick={() => {
              stopAll();
              props.setStatus("done");
            }}
            style={{
              padding: "10px 14px",
              borderRadius: 12,
              border: "1px solid rgba(0,0,0,0.12)",
              background: canStop ? "white" : "rgba(0,0,0,0.05)",
              cursor: canStop ? "pointer" : "not-allowed",
              fontSize: 13,
              fontWeight: 600,
            }}
          >
            â¹ ç»“æŸ
          </button>

          <div style={{ fontSize: 12, padding: "6px 10px", borderRadius: 999, background: "rgba(0,0,0,0.06)" }}>
            {statusMap[props.status] ?? props.status}
          </div>
        </div>
      </div>

      <div style={{ marginTop: 12, padding: 12, borderRadius: 12, background: "rgba(0,0,0,0.04)" }}>
        <div style={{ fontSize: 13, opacity: 0.75 }}>
        ç½‘ç»œè¿æ¥ä¸ç¨³å®šæ—¶ï¼Œä¼šåˆ‡æ¢æ¨¡å¼ã€‚
        </div>
      </div>

      <div style={{ marginTop: 12 }}>
        <div style={{ fontWeight: 600, marginBottom: 6 }}>é€å­—ç¨¿ï¼ˆæœ€ç»ˆå®šç¨¿ï¼‰</div>
        <div style={{ maxHeight: 240, overflow: "auto", fontSize: 13, lineHeight: 1.6 }}>
          {props.turns.length === 0 ? (
            <div style={{ opacity: 0.6 }}>è¿˜æ²¡æœ‰å¯¹è¯å†…å®¹ã€‚</div>
          ) : (
            props.turns.map((t, i) => (
              <div key={i} style={{ marginBottom: 8 }}>
                <b>{t.role === "user" ? "ä½ " : "AI"}ï¼š</b>
                {t.text}
              </div>
            ))
          )}
        </div>
      </div>
    </div>
  );
}
